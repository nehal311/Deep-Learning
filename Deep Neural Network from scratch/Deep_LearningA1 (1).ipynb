{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMEjKJ7kTu1I",
        "outputId": "0c1cbdf5-83b8-4756-d0ec-b375cc90708a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c616CMaNfP0",
        "outputId": "a9842e37-0647-40c7-97da-82f714e858c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as et\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "import xgboost\n",
        "import six\n",
        "import sys\n",
        "import keras\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "import imblearn\n",
        "\n",
        "# Packages for data preparation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Packages for modeling\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLc_H2vj8kdE",
        "outputId": "6e2479eb-afde-448c-aff6-aeb7552286a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import math,nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import  hstack\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, concatenate,Flatten,LSTM, Embedding, Input,Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split \n",
        "import itertools \n",
        "from sklearn.preprocessing import LabelBinarizer,label_binarize\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout,Embedding,LSTM\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,r2_score,mean_squared_error,roc_auc_score,roc_curve,auc, f1_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
        "from keras import regularizers\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "op0_SxKvTtJm",
        "outputId": "26726399-8d38-44b7-a48c-aa17c0d47136"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0             This computer is absolutely AMAZING!!!   \n",
              "1                        10 plus hours of battery...   \n",
              "2  super fast processor and really nice graphics ...   \n",
              "3  super fast processor and really nice graphics ...   \n",
              "4  and plenty of storage with 250 gb(though I wil...   \n",
              "5  This computer is really fast and I'm shocked a...   \n",
              "6  This computer is really fast and I'm shocked a...   \n",
              "7  I've only had mine a day but I'm already used ...   \n",
              "8  GET THIS COMPUTER FOR PORTABILITY AND FAST PRO...   \n",
              "9  GET THIS COMPUTER FOR PORTABILITY AND FAST PRO...   \n",
              "\n",
              "                 aspect category  polarity  \n",
              "0                 LAPTOP#GENERAL  positive  \n",
              "1  BATTERY#OPERATION_PERFORMANCE  positive  \n",
              "2      CPU#OPERATION_PERFORMANCE  positive  \n",
              "3               GRAPHICS#GENERAL  positive  \n",
              "4      HARD_DISC#DESIGN_FEATURES  positive  \n",
              "5   LAPTOP#OPERATION_PERFORMANCE  positive  \n",
              "6               LAPTOP#USABILITY  positive  \n",
              "7               LAPTOP#USABILITY  positive  \n",
              "8             LAPTOP#PORTABILITY  positive  \n",
              "9      CPU#OPERATION_PERFORMANCE  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c58c7119-9dc0-4fe6-86d0-dc889624a8fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect category</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This computer is absolutely AMAZING!!!</td>\n",
              "      <td>LAPTOP#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10 plus hours of battery...</td>\n",
              "      <td>BATTERY#OPERATION_PERFORMANCE</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>super fast processor and really nice graphics ...</td>\n",
              "      <td>CPU#OPERATION_PERFORMANCE</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>super fast processor and really nice graphics ...</td>\n",
              "      <td>GRAPHICS#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and plenty of storage with 250 gb(though I wil...</td>\n",
              "      <td>HARD_DISC#DESIGN_FEATURES</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>This computer is really fast and I'm shocked a...</td>\n",
              "      <td>LAPTOP#OPERATION_PERFORMANCE</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>This computer is really fast and I'm shocked a...</td>\n",
              "      <td>LAPTOP#USABILITY</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I've only had mine a day but I'm already used ...</td>\n",
              "      <td>LAPTOP#USABILITY</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GET THIS COMPUTER FOR PORTABILITY AND FAST PRO...</td>\n",
              "      <td>LAPTOP#PORTABILITY</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GET THIS COMPUTER FOR PORTABILITY AND FAST PRO...</td>\n",
              "      <td>CPU#OPERATION_PERFORMANCE</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c58c7119-9dc0-4fe6-86d0-dc889624a8fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c58c7119-9dc0-4fe6-86d0-dc889624a8fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c58c7119-9dc0-4fe6-86d0-dc889624a8fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "xml = '/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/Data/ABSA16_Laptops_Train_SB1_v2.xml'\n",
        "  \n",
        "\n",
        "def xml2datadrame(xml):\n",
        "    dataset = []                                              \n",
        "    root = et.parse(xml).getroot()  \n",
        "\n",
        "    for i in root:\n",
        "      for review in i:\n",
        "        for sen in review:\n",
        "          try:\n",
        "            for txt1 in sen.iter('text'):\n",
        "              t = txt1.text\n",
        "            for txt1 in sen.iter('Opinion'):\n",
        "                category = txt1.attrib[\"category\"]\n",
        "                polarity = txt1.attrib[\"polarity\"]\n",
        "                #aspect = txt1.attrib[\"target\"]\n",
        "\n",
        "                row = {\"text\": t,\"aspect category\":category,\"polarity\":polarity}   \n",
        "                dataset.append(row)\n",
        "\n",
        "          except IndexError: \n",
        "              row = {\"sentence\": t}        \n",
        "              dataset.append(row)  \n",
        "          \n",
        "    return pd.DataFrame(dataset)\n",
        "           \n",
        "\n",
        "Laptop_df = xml2datadrame(xml)\n",
        "Laptop_df_new = Laptop_df[Laptop_df['text'].str.contains(\"\\n\") == False]\n",
        "Laptop_df_new.head(10)\n",
        "#len(Laptop_df_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "meIy144duFyr",
        "outputId": "115814f3-1046-46af-d165-6b72fa587ca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  Well, my first apple computer and I am impressed.\n",
              "1                   Works well, fast and no reboots.\n",
              "2  Waiting to install MS Office and see how it go...\n",
              "3  Have always been a PC guy, but decided to try ...\n",
              "4                                 Glad I did so far.\n",
              "5  s.... L .... o..... w....  rea......llllyy  slow.\n",
              "6                     like seriously  - really slow.\n",
              "7                                 impossible to use.\n",
              "8                           cant even read properly.\n",
              "9                    plus - no russian input ??  wtf"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-721536f9-0e08-493f-a3b8-a7095976b423\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well, my first apple computer and I am impressed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Works well, fast and no reboots.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Waiting to install MS Office and see how it go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Have always been a PC guy, but decided to try ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Glad I did so far.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>s.... L .... o..... w....  rea......llllyy  slow.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>like seriously  - really slow.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>impossible to use.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cant even read properly.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>plus - no russian input ??  wtf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-721536f9-0e08-493f-a3b8-a7095976b423')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-721536f9-0e08-493f-a3b8-a7095976b423 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-721536f9-0e08-493f-a3b8-a7095976b423');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "xml = '/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/Data/EN_LAPT_SB1_TEST_.xml'\n",
        "  \n",
        "def xml2datadrame(xml):\n",
        "    dataset = []                                              \n",
        "    root = et.parse(xml).getroot()  \n",
        "\n",
        "    for i in root:\n",
        "      for review in i:\n",
        "        for sen in review:\n",
        "          try:\n",
        "            for txt1 in sen.iter('text'):\n",
        "              t = txt1.text\n",
        "\n",
        "            row = {\"text\": t}   \n",
        "            dataset.append(row)\n",
        "\n",
        "          except IndexError: \n",
        "              row = {\"sentence\": t}        \n",
        "              dataset.append(row)  \n",
        "          \n",
        "    return pd.DataFrame(dataset)\n",
        "           \n",
        "\n",
        "Laptop_df = xml2datadrame(xml)\n",
        "test = Laptop_df[Laptop_df['text'].str.contains(\"\\n\") == False]\n",
        "test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB6WegrtTs--",
        "outputId": "610e610c-7ead-498f-e9ad-93341a19b018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2909 entries, 0 to 2908\n",
            "Data columns (total 3 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   text             2909 non-null   object\n",
            " 1   aspect category  2909 non-null   object\n",
            " 2   polarity         2909 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 90.9+ KB\n"
          ]
        }
      ],
      "source": [
        "Laptop_df_new.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaDJROyrTs8F"
      },
      "outputs": [],
      "source": [
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 6, 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "CANl6Dl3Ts40",
        "outputId": "934243db-348a-480c-c73e-324c94db07c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(Laptop_df_new.polarity)\n",
        "plt.xlabel('Polarity');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GHjPw6QTs1N"
      },
      "outputs": [],
      "source": [
        "#handling categorical value : Converting the polarity into integers-1(Negative), 0(Neutral) and 1(positive) sentiment:\n",
        "def to_sentiment(rating):\n",
        "  #rating = int(rating)\n",
        "  if rating == \"positive\":\n",
        "    return 1\n",
        "  elif rating == \"negative\":\n",
        "    return -1\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "Laptop_df_new['polarity'] = Laptop_df_new.polarity.apply(to_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DT-7VJ3BTsrU",
        "outputId": "d60a3385-ddc0-451c-85d7-f82e4b47dabd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0                This computer is absolutely AMAZING!!!   \n",
              "1                           10 plus hours of battery...   \n",
              "2     super fast processor and really nice graphics ...   \n",
              "3     super fast processor and really nice graphics ...   \n",
              "4     and plenty of storage with 250 gb(though I wil...   \n",
              "...                                                 ...   \n",
              "2904  I would recommend it, for anybody needing a re...   \n",
              "2905  I bought this for my daughter to use for schoo...   \n",
              "2906  I bought this for my daughter to use for schoo...   \n",
              "2907  I bought this for my daughter to use for schoo...   \n",
              "2908                                Love the price too!   \n",
              "\n",
              "                    aspect category  polarity  \n",
              "0                    LAPTOP#GENERAL         1  \n",
              "1     BATTERY#OPERATION_PERFORMANCE         1  \n",
              "2         CPU#OPERATION_PERFORMANCE         1  \n",
              "3                  GRAPHICS#GENERAL         1  \n",
              "4         HARD_DISC#DESIGN_FEATURES         1  \n",
              "...                             ...       ...  \n",
              "2904                 LAPTOP#QUALITY         1  \n",
              "2905   LAPTOP#OPERATION_PERFORMANCE         0  \n",
              "2906         LAPTOP#DESIGN_FEATURES         1  \n",
              "2907                 LAPTOP#GENERAL         1  \n",
              "2908                   LAPTOP#PRICE         1  \n",
              "\n",
              "[2909 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8e5d9db-0d96-486d-8643-1ec06e533d54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect category</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This computer is absolutely AMAZING!!!</td>\n",
              "      <td>LAPTOP#GENERAL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10 plus hours of battery...</td>\n",
              "      <td>BATTERY#OPERATION_PERFORMANCE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>super fast processor and really nice graphics ...</td>\n",
              "      <td>CPU#OPERATION_PERFORMANCE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>super fast processor and really nice graphics ...</td>\n",
              "      <td>GRAPHICS#GENERAL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and plenty of storage with 250 gb(though I wil...</td>\n",
              "      <td>HARD_DISC#DESIGN_FEATURES</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2904</th>\n",
              "      <td>I would recommend it, for anybody needing a re...</td>\n",
              "      <td>LAPTOP#QUALITY</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#OPERATION_PERFORMANCE</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#DESIGN_FEATURES</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2907</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#GENERAL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>Love the price too!</td>\n",
              "      <td>LAPTOP#PRICE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2909 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8e5d9db-0d96-486d-8643-1ec06e533d54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8e5d9db-0d96-486d-8643-1ec06e533d54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8e5d9db-0d96-486d-8643-1ec06e533d54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "Laptop_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf30lUPnE8mK",
        "outputId": "37f151d1-9e17-400d-b656-99fd18c4266a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LAPTOP#GENERAL                  634\n",
              "LAPTOP#OPERATION_PERFORMANCE    278\n",
              "LAPTOP#DESIGN_FEATURES          253\n",
              "LAPTOP#QUALITY                  224\n",
              "LAPTOP#MISCELLANEOUS            142\n",
              "                               ... \n",
              "FANS_COOLING#DESIGN_FEATURES      1\n",
              "SOFTWARE#PRICE                    1\n",
              "CPU#MISCELLANEOUS                 1\n",
              "PORTS#DESIGN_FEATURES             1\n",
              "CPU#DESIGN_FEATURES               1\n",
              "Name: aspect category, Length: 81, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "Laptop_df_new['aspect category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmmhe-x6gYYq"
      },
      "outputs": [],
      "source": [
        "# Get the count of each value\n",
        "value_counts = Laptop_df_new['aspect category'].value_counts()\n",
        "\n",
        "# Select the values where the count is less than 3 (or 5 if you like)\n",
        "to_remove = value_counts[value_counts <= 20].index\n",
        "\n",
        "# Keep rows where the city column is not in to_remove\n",
        "Laptop_df_new = Laptop_df_new[~Laptop_df_new['aspect category'].isin(to_remove)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaNklNRHoHp1",
        "outputId": "bcd8c74d-83b7-449c-ac7a-6a497acc1ee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2547, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Laptop_df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdtQ1b_5ntP8",
        "outputId": "27a663ca-35ef-4acb-b6ea-6b90043bec41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LAPTOP#GENERAL                   634\n",
              "LAPTOP#OPERATION_PERFORMANCE     278\n",
              "LAPTOP#DESIGN_FEATURES           253\n",
              "LAPTOP#QUALITY                   224\n",
              "LAPTOP#MISCELLANEOUS             142\n",
              "LAPTOP#USABILITY                 141\n",
              "SUPPORT#QUALITY                  138\n",
              "LAPTOP#PRICE                     136\n",
              "COMPANY#GENERAL                   90\n",
              "BATTERY#OPERATION_PERFORMANCE     86\n",
              "LAPTOP#CONNECTIVITY               55\n",
              "DISPLAY#QUALITY                   53\n",
              "LAPTOP#PORTABILITY                51\n",
              "OS#GENERAL                        35\n",
              "SOFTWARE#GENERAL                  31\n",
              "KEYBOARD#DESIGN_FEATURES          29\n",
              "MOUSE#OPERATION_PERFORMANCE       28\n",
              "DISPLAY#DESIGN_FEATURES           28\n",
              "MULTIMEDIA_DEVICES#QUALITY        26\n",
              "DISPLAY#GENERAL                   24\n",
              "MOUSE#USABILITY                   23\n",
              "KEYBOARD#USABILITY                21\n",
              "OS#USABILITY                      21\n",
              "Name: aspect category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "Laptop_df_new['aspect category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCLr1ZQfTsn8"
      },
      "outputs": [],
      "source": [
        "#Drop Nan Values\n",
        "Laptop_df_new=Laptop_df_new.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Laptop_df_new['aspect category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrAXystoQSRM",
        "outputId": "5d0dffb1-ba85-4c30-89ec-de4e7b66988c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPwPlkdOTsks",
        "outputId": "f326923f-8698-4e0b-a876-f5dfe3690d79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text               0\n",
              "aspect category    0\n",
              "polarity           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#Checking for null values\n",
        "Laptop_df_new.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiN4cvLBu6Hq",
        "outputId": "493fadc3-78cc-4a39-d12b-4ffd7cda4481"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'aspect category', 'polarity'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "data = Laptop_df_new\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR842ez9IChr"
      },
      "outputs": [],
      "source": [
        "def clean(tweet): \n",
        "    \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    \n",
        "    #emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    tweet =  emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    # usernames mentions like \"@abc123\"\n",
        "    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n",
        "    tweet =  ment.sub(r'', tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # html tags\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    tweet = re.sub(html, '', tweet)\n",
        "    \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+','', tweet)\n",
        "        \n",
        "    #Punctuations and special characters\n",
        "    \n",
        "    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n",
        "    \n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    splits = tweet.split()\n",
        "    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    tweet = ' '.join(splits)\n",
        "    \n",
        "    \n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Lw3bb8rMICdt",
        "outputId": "61ecba7f-a088-4b17-8f7f-f973e1a274a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0                This computer is absolutely AMAZING!!!   \n",
              "1                           10 plus hours of battery...   \n",
              "5     This computer is really fast and I'm shocked a...   \n",
              "6     This computer is really fast and I'm shocked a...   \n",
              "7     I've only had mine a day but I'm already used ...   \n",
              "...                                                 ...   \n",
              "2904  I would recommend it, for anybody needing a re...   \n",
              "2905  I bought this for my daughter to use for schoo...   \n",
              "2906  I bought this for my daughter to use for schoo...   \n",
              "2907  I bought this for my daughter to use for schoo...   \n",
              "2908                                Love the price too!   \n",
              "\n",
              "                    aspect category  polarity  \\\n",
              "0                    LAPTOP#GENERAL         1   \n",
              "1     BATTERY#OPERATION_PERFORMANCE         1   \n",
              "5      LAPTOP#OPERATION_PERFORMANCE         1   \n",
              "6                  LAPTOP#USABILITY         1   \n",
              "7                  LAPTOP#USABILITY         1   \n",
              "...                             ...       ...   \n",
              "2904                 LAPTOP#QUALITY         1   \n",
              "2905   LAPTOP#OPERATION_PERFORMANCE         0   \n",
              "2906         LAPTOP#DESIGN_FEATURES         1   \n",
              "2907                 LAPTOP#GENERAL         1   \n",
              "2908                   LAPTOP#PRICE         1   \n",
              "\n",
              "                                           cleaned_text  \n",
              "0                           computer absolutely amazing  \n",
              "1                                 10 plus hours battery  \n",
              "5            computer really fast shocked easy get used  \n",
              "6            computer really fast shocked easy get used  \n",
              "7                                 mine day already used  \n",
              "...                                                 ...  \n",
              "2904  would recommend anybody needing reliable simpl...  \n",
              "2905  bought daughter use school homework fast compu...  \n",
              "2906  bought daughter use school homework fast compu...  \n",
              "2907  bought daughter use school homework fast compu...  \n",
              "2908                                         love price  \n",
              "\n",
              "[2547 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-795f7406-d63b-45b4-be45-f1ed763de4c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect category</th>\n",
              "      <th>polarity</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This computer is absolutely AMAZING!!!</td>\n",
              "      <td>LAPTOP#GENERAL</td>\n",
              "      <td>1</td>\n",
              "      <td>computer absolutely amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10 plus hours of battery...</td>\n",
              "      <td>BATTERY#OPERATION_PERFORMANCE</td>\n",
              "      <td>1</td>\n",
              "      <td>10 plus hours battery</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>This computer is really fast and I'm shocked a...</td>\n",
              "      <td>LAPTOP#OPERATION_PERFORMANCE</td>\n",
              "      <td>1</td>\n",
              "      <td>computer really fast shocked easy get used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>This computer is really fast and I'm shocked a...</td>\n",
              "      <td>LAPTOP#USABILITY</td>\n",
              "      <td>1</td>\n",
              "      <td>computer really fast shocked easy get used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I've only had mine a day but I'm already used ...</td>\n",
              "      <td>LAPTOP#USABILITY</td>\n",
              "      <td>1</td>\n",
              "      <td>mine day already used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2904</th>\n",
              "      <td>I would recommend it, for anybody needing a re...</td>\n",
              "      <td>LAPTOP#QUALITY</td>\n",
              "      <td>1</td>\n",
              "      <td>would recommend anybody needing reliable simpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#OPERATION_PERFORMANCE</td>\n",
              "      <td>0</td>\n",
              "      <td>bought daughter use school homework fast compu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#DESIGN_FEATURES</td>\n",
              "      <td>1</td>\n",
              "      <td>bought daughter use school homework fast compu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2907</th>\n",
              "      <td>I bought this for my daughter to use for schoo...</td>\n",
              "      <td>LAPTOP#GENERAL</td>\n",
              "      <td>1</td>\n",
              "      <td>bought daughter use school homework fast compu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>Love the price too!</td>\n",
              "      <td>LAPTOP#PRICE</td>\n",
              "      <td>1</td>\n",
              "      <td>love price</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2547 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-795f7406-d63b-45b4-be45-f1ed763de4c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-795f7406-d63b-45b4-be45-f1ed763de4c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-795f7406-d63b-45b4-be45-f1ed763de4c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data['cleaned_text']= data['text'].apply((lambda x: clean(x))) \n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz1bNinql0XY"
      },
      "source": [
        "ASPECT CATEGORY CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30YRx0ZFlv6Q"
      },
      "source": [
        "CNN_ASPECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASeli2XZxRud",
        "outputId": "15c1ac4f-d555-4de0-819b-d634e7af9796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train data samples: 2292\n",
            "# Test data samples: 255\n",
            "Fitted tokenizer on 2292 documents\n",
            "5000 words in dictionary\n",
            "Top 5 most common words are: [('laptop', 313), ('computer', 264), ('great', 243), ('use', 186), ('would', 145)]\n",
            "\"[165, 138, 14, 126, 2]\" is converted into [0. 0. 1. ... 0. 0. 0.]\n",
            "For this example we have 5.0 features with a value of 1.\n",
            "(12857, 5000)\n",
            "(12857,)\n",
            "Shape of validation set: (1286, 5000)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 64)                320064    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 325,069\n",
            "Trainable params: 325,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0c23c07110d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                        , verbose=1)\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel_cnn_ac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/CNN_Aspect.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 23) and (None, 13) are incompatible\n"
          ]
        }
      ],
      "source": [
        "#CNN_ASPECT\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['aspect category'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000 \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =1  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le1 = LabelEncoder()\n",
        "y_train_le = le1.fit_transform(y_train)\n",
        "y_test_le = le1.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "#base_model\n",
        "base_model = models.Sequential()\n",
        "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "base_model.add(layers.Dense(64, activation='relu'))\n",
        "base_model.add(layers.Dense(13, activation='softmax'))\n",
        "base_model.summary()\n",
        "\n",
        "base_model.compile(optimizer='adam'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "model_cnn_ac = base_model    \n",
        "history = model_cnn_ac.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_cnn_ac.save('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/CNN_Aspect.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UQAOD1A3GjD"
      },
      "outputs": [],
      "source": [
        "cnn_ac_pred = model_cnn_ac.predict(X_test_oh)\n",
        "cnn_ac_pred = np.argmax(cnn_ac_pred,axis=1)\n",
        "cnn_ac_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(cnn_ac_pred, cnn_ac_org))\n",
        "\n",
        "cmat = confusion_matrix(cnn_ac_pred, cnn_ac_org)\n",
        "#plt.figure(figsize=(6,6))\n",
        "plt.figure(figsize=(16,7))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['LAPTOP#GENERAL','LAPTOP#OPERATION_PERFORMANCE' ,'LAPTOP#DESIGN_FEATURES ',\n",
        "              'LAPTOP#QUALITY', 'LAPTOP#MISCELLANEOUS', 'LAPTOP#USABILITY' ,\n",
        "              'SUPPORT#QUALITY' ,'LAPTOP#PRICE' ,'COMPANY#GENERAL','BATTERY#OPERATION_PERFORMANCE',\n",
        "              'LAPTOP#CONNECTIVITY','DISPLAY#QUALITY' ,'LAPTOP#PORTABILITY']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n",
        "#sns.heatmap(cmat, annot = True, cbar = False, cmap='Paired', fmt=\"d\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvE1PJBX8iPh"
      },
      "source": [
        "LSTM_ASPECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vdr-j3IGHcHC",
        "outputId": "98221971-c789-4054-e02f-a8c1afa77803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train data samples: 2292\n",
            "# Test data samples: 255\n",
            "Fitted tokenizer on 2292 documents\n",
            "5000 words in dictionary\n",
            "Top 5 most common words are: [('laptop', 322), ('computer', 263), ('great', 242), ('use', 184), ('would', 144)]\n",
            "\"[257, 1634, 258, 8, 116, 1635, 690]\" is converted into [0. 0. 0. ... 0. 0. 0.]\n",
            "For this example we have 7.0 features with a value of 1.\n",
            "(13317, 5000)\n",
            "(13317,)\n",
            "Shape of validation set: (1332, 5000)\n",
            "Number of unique words: 2609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2609/2609 [00:00<00:00, 418692.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5000, 300)         783000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 5000, 100)         160400    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 500000)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500000)            0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                32000064  \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,948,469\n",
            "Trainable params: 32,165,469\n",
            "Non-trainable params: 783,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0e5e38779524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                        , verbose=1)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mmodel_lstm_ac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/LSTM_Aspect.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 23) and (None, 13) are incompatible\n"
          ]
        }
      ],
      "source": [
        "#LSTM\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['aspect category'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000  \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =10  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "X_train_oh.shape\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "embed_len = 300\n",
        "\n",
        "embedding_dict={}\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/glove.6B.300d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "global vocab_size\n",
        "vocab_size= len(tk.word_index)+1\n",
        "vocab_size\n",
        "\n",
        "word_index=tk.word_index\n",
        "print('Number of unique words:',len(word_index))\n",
        "\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,300))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size,embed_len,embeddings_initializer=keras.initializers.Constant(embedding_matrix),input_length=NB_WORDS,trainable=False))\n",
        "model.add(LSTM(100,return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(13,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "model.summary()\n",
        "model_lstm_ac = model\n",
        "history = model_lstm_ac.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_lstm_ac.save(\"/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/LSTM_Aspect.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g2EN3oT_Qoi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9FbyMNX5UwS"
      },
      "outputs": [],
      "source": [
        "lstm_ac_pred = model_lstm_ac.predict(X_test_oh)\n",
        "lstm_ac_pred = np.argmax(lstm_ac_pred,axis=1)\n",
        "lstm_ac_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(lstm_ac_pred, lstm_ac_org))\n",
        "\n",
        "cmat = confusion_matrix(lstm_ac_pred, lstm_ac_org)\n",
        "plt.figure(figsize=(16,7))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['LAPTOP#GENERAL','LAPTOP#OPERATION_PERFORMANCE' ,'LAPTOP#DESIGN_FEATURES ',\n",
        "              'LAPTOP#QUALITY', 'LAPTOP#MISCELLANEOUS', 'LAPTOP#USABILITY' ,\n",
        "              'SUPPORT#QUALITY' ,'LAPTOP#PRICE' ,'COMPANY#GENERAL','BATTERY#OPERATION_PERFORMANCE',\n",
        "              'LAPTOP#CONNECTIVITY','DISPLAY#QUALITY' ,'LAPTOP#PORTABILITY']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu8f4ly2_mM4"
      },
      "source": [
        "BiLSTM_ASPECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xyjLLksxC2g"
      },
      "outputs": [],
      "source": [
        "#BILSTM\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['aspect category'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000  \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =10  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "X_train_oh.shape\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "embed_len = 300\n",
        "\n",
        "embedding_dict={}\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/glove.6B.300d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "global vocab_size\n",
        "vocab_size= len(tk.word_index)+1\n",
        "vocab_size\n",
        "\n",
        "word_index=tk.word_index\n",
        "print('Number of unique words:',len(word_index))\n",
        "\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,300))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size,embed_len,embeddings_initializer=keras.initializers.Constant(embedding_matrix),input_length=NB_WORDS,trainable=False))\n",
        "model.add(keras.layers.Bidirectional(LSTM(100,return_sequences=True)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dense(13,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "model.summary()\n",
        "model_bilstm_ac = model\n",
        "history = model_bilstm_ac.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_bilstm_ac.save('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/BILSTM_Aspect.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USAN5i9p6TFm"
      },
      "outputs": [],
      "source": [
        "bilstm_ac_pred = model_bilstm_ac.predict(X_test_oh)\n",
        "bilstm_ac_pred = np.argmax(bilstm_ac_pred,axis=1)\n",
        "bilstm_ac_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(bilstm_ac_pred, bilstm_ac_org))\n",
        "\n",
        "cmat = confusion_matrix(bilstm_ac_pred, bilstm_ac_org)\n",
        "plt.figure(figsize=(16,7))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['LAPTOP#GENERAL','LAPTOP#OPERATION_PERFORMANCE' ,'LAPTOP#DESIGN_FEATURES ',\n",
        "              'LAPTOP#QUALITY', 'LAPTOP#MISCELLANEOUS', 'LAPTOP#USABILITY' ,\n",
        "              'SUPPORT#QUALITY' ,'LAPTOP#PRICE' ,'COMPANY#GENERAL','BATTERY#OPERATION_PERFORMANCE',\n",
        "              'LAPTOP#CONNECTIVITY','DISPLAY#QUALITY' ,'LAPTOP#PORTABILITY']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRnQ7lXiWnQ"
      },
      "source": [
        "Polarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6KJogimdO1"
      },
      "source": [
        "CNN_POLARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is66EwYiJis7"
      },
      "outputs": [],
      "source": [
        "#CNN\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['polarity'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000  \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =10  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le2 = LabelEncoder()\n",
        "y_train_le = le2.fit_transform(y_train)\n",
        "y_test_le = le2.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "#base_model\n",
        "base_model = models.Sequential()\n",
        "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "base_model.add(layers.Dense(64, activation='relu'))\n",
        "base_model.add(layers.Dense(3, activation='softmax'))\n",
        "base_model.summary()\n",
        "\n",
        "base_model.compile(optimizer='adam'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "model_cnn_pol = base_model   \n",
        "history = model_cnn_pol.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_cnn_pol.save('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/sentiment_polarity_model/CNN_polarity.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k7usOBg7vwG"
      },
      "outputs": [],
      "source": [
        "cnn_pol_pred = model_cnn_pol.predict(X_test_oh)\n",
        "cnn_pol_pred = np.argmax(cnn_pol_pred,axis=1)\n",
        "cnn_pol_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(cnn_pol_pred, cnn_pol_org))\n",
        "\n",
        "cmat = confusion_matrix(cnn_pol_pred, cnn_pol_org)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['Positive','Negative' ,'Neutral ']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67H2EEnUm6X3"
      },
      "source": [
        "LSTM_POLARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9EZXilpm-Pa"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['polarity'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000  \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =10  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "X_train_oh.shape\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "embed_len = 300\n",
        "\n",
        "embedding_dict={}\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/glove.6B.300d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "global vocab_size\n",
        "vocab_size= len(tk.word_index)+1\n",
        "vocab_size\n",
        "\n",
        "word_index=tk.word_index\n",
        "print('Number of unique words:',len(word_index))\n",
        "\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,300))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size,embed_len,embeddings_initializer=keras.initializers.Constant(embedding_matrix),input_length=NB_WORDS,trainable=False))\n",
        "model.add(LSTM(100,return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "model.summary()\n",
        "model_lstm_pol = model\n",
        "history = model_lstm_pol.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_lstm_pol.save('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/sentiment_polarity_model/LSTM_polarity.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISiU_ZmC8bgm"
      },
      "outputs": [],
      "source": [
        "lstm_pol_pred = model_lstm_pol.predict(X_test_oh)\n",
        "lstm_pol_pred = np.argmax(lstm_pol_pred,axis=1)\n",
        "lstm_pol_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(lstm_pol_pred, lstm_pol_org))\n",
        "\n",
        "cmat = confusion_matrix(lstm_pol_pred, lstm_pol_org)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['Positive','Negative' ,'Neutral ']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2NiKUZInKQx"
      },
      "source": [
        "BILSTM_POLARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdvYJVwhnLmN"
      },
      "outputs": [],
      "source": [
        "#BILSTM\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['polarity'], test_size=0.1)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "NB_WORDS = 5000  \n",
        "VAL_SIZE = 1000 \n",
        "NB_START_EPOCHS =10  \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "X_train_oh = one_hot_seq(X_train_seq)\n",
        "X_test_oh = one_hot_seq(X_test_seq)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
        "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))\n",
        "\n",
        "X_train_oh.shape\n",
        "\n",
        "#oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X_train_oh, y_train)\n",
        "\n",
        "X_train_oh = X\n",
        "y_train = y\n",
        "print(X_train_oh.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)\n",
        "\n",
        "embed_len = 300\n",
        "\n",
        "embedding_dict={}\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/glove.6B.300d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "global vocab_size\n",
        "vocab_size= len(tk.word_index)+1\n",
        "vocab_size\n",
        "\n",
        "word_index=tk.word_index\n",
        "print('Number of unique words:',len(word_index))\n",
        "\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,300))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size,embed_len,embeddings_initializer=keras.initializers.Constant(embedding_matrix),input_length=NB_WORDS,trainable=False))\n",
        "model.add(keras.layers.Bidirectional(LSTM(100,return_sequences=True)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "model.summary()\n",
        "model_bilstm_pol = model\n",
        "history = model_bilstm_pol.fit(X_train_rest, y_train_rest\n",
        "                       , epochs=10\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=1)\n",
        "\n",
        "model_bilstm_pol.save('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/sentiment_polarity_model/BILSTM_polarity.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSYanp9p8274"
      },
      "outputs": [],
      "source": [
        "bilstm_pol_pred = model_bilstm_pol.predict(X_test_oh)\n",
        "bilstm_pol_pred = np.argmax(bilstm_pol_pred,axis=1)\n",
        "bilstm_pol_org = np.argmax(y_test_oh,axis=1)\n",
        "\n",
        "print(classification_report(bilstm_pol_pred, bilstm_pol_org))\n",
        "\n",
        "cmat = confusion_matrix(bilstm_pol_pred, bilstm_pol_org)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.set(font_scale = 1.4)\n",
        "sns.heatmap(cmat, annot = True, cbar = False, cmap=plt.cm.Greens, fmt=\"d\",linewidths=0.2);\n",
        "class_names = ['Positive','Negative' ,'Neutral ']\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhj9ifPLoF91"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-W8zLnY_q4R"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# loading tokenizer\n",
        "with open('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/tokenizer.pickle', 'rb') as handle:\n",
        "    tk = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED3ZMzwn_3gQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_cnn_ac = load_model('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/aspect_category_model/CNN_Aspect.h5')\n",
        "model_cnn_pol = load_model('/content/drive/MyDrive/Aspect_Based_Sentiment_Analysis/models/sentiment_polarity_model/CNN_polarity.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvxT9Yu1IZoC"
      },
      "outputs": [],
      "source": [
        "def clean(tweet): \n",
        "    \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    \n",
        "    #emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    tweet =  emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    # usernames mentions like \"@abc123\"\n",
        "    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n",
        "    tweet =  ment.sub(r'', tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # html tags\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    tweet = re.sub(html, '', tweet)\n",
        "    \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+','', tweet)\n",
        "        \n",
        "    #Punctuations and special characters\n",
        "    \n",
        "    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n",
        "    \n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    splits = tweet.split()\n",
        "    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    tweet = ' '.join(splits)\n",
        "    \n",
        "    \n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K6u4wnjoHMc"
      },
      "outputs": [],
      "source": [
        "NB_WORDS=5000\n",
        "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
        "    ohs = np.zeros((len(seqs), nb_features))\n",
        "    for i, s in enumerate(seqs):\n",
        "        ohs[i, s] = 1.\n",
        "    return ohs\n",
        "\n",
        "def test_aspect_sentiment(text):\n",
        "  df = pd.DataFrame(text,columns=['text'])\n",
        "  text1 = df['text'].apply((lambda x: clean(x)))\n",
        "  #print(text1.tolist()) \n",
        "  X_testing_seq = tk.texts_to_sequences(text1)\n",
        "  X_testing_oh = one_hot_seq(X_testing_seq)\n",
        "\n",
        "  ac = model_cnn_ac.predict(X_testing_oh)\n",
        "  pol = model_cnn_pol.predict(X_testing_oh)\n",
        "  ac = np.argmax(ac,axis=1)\n",
        "  pol = np.argmax(pol,axis=1)\n",
        "  ac = le1.inverse_transform(ac)\n",
        "  pol = le2.inverse_transform(pol)\n",
        "  pol1 = []\n",
        "  for i in pol:\n",
        "    if i == 1:\n",
        "      pol1.append(\"positive\")\n",
        "    elif i == -1:\n",
        "      pol1.append(\"negative\")\n",
        "    else: \n",
        "      pol1.append(\"netural\")\n",
        "  df1 = pd.DataFrame({'text':text1,'aspect_category':ac,'polarity':pol1})\n",
        "  print(df1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJHtcVHFyjU8"
      },
      "outputs": [],
      "source": [
        "text = ['Well, my first apple computer and I am impressed','Have always been a PC guy, but decided to try Apple','cant even read properly']\n",
        "test_aspect_sentiment(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}