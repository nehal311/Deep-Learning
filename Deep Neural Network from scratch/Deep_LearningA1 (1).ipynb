{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_LearningA1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Assignment 1**\n",
        "**Nehal Malap - 21237207 - CSD\n",
        "Gehna Gurbani - 21230450 -CSD** \n",
        "\n",
        "The aim of the assignment is to implement a deep learning neural network of arbitrary layers from scratch and test it on given datasets. The report contains five tasks: \n",
        "Task 1: Implementation of Logistic Regression from scratch. \n",
        "Task 2: Test the build logistic model on given datasets(circle600 and blobs300).Task 3: Implementation of shallow Neural Network. \n",
        "Task 4: Image Recognition of given category(Dog and Automobile). \n",
        "Task 5: Implementation of neural network with an arbitrary number of layers and Deep learning enhancements."
      ],
      "metadata": {
        "id": "IUwwDv2iXtiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1: Implementation of Logistic Regression**\n",
        "\n",
        "**Logistic Regression:** It is a supervised learning classification algorithm that produces binary results of categorical dependent variable based on the concept of probability. Then the same probability used to classify the data with the help of activation function(sigmoid function). Logistic Regression widely used in medical fields, and social sciences.[1]\n",
        "\n",
        "**Implemtation Details**:\n",
        "\n",
        "The *best fit line equation* of LR is y=ax+b, where\n",
        "a= multiplication of the input vector and its corresponding weight vector\n",
        "b= bias term\n",
        "The output of the best fine converted into 0 and 1 and fed to the sigmoid function that returns the y_hat value.\n",
        "\n",
        "The *sigmoid function* = f(z) = 1 / (1 + np.exp(-z)).The output of the sigmoid function compared with the threshold value as if that\n",
        "\n",
        "*y_hat â‰¥ 0.5, then set it as 1 y_hat â©½ 0.5, then set it as 0*. \n",
        "\n",
        "The *Loss Function* in LR is log loss function. It is error representation of the model. Hence, greater the cost function, lesser the accuracy of the machine learning model. applying mean squared error on a logistic regression model returns multiple local minimums of the cost rather than a single global minimum.\n",
        "The cost can be calculated as follows: \n",
        "\n",
        "J = -(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
        "\n",
        "The *Stochastic Gradient Decent*  algorithm is used to minimise the cost function of the logistic regression model, in order to achieve the highest accuracy possible[2]\n",
        "\n",
        "ð‘¾ = ð‘¾ âˆ’ ðœ¶ (ðð‘ªð’ð’”ð’•/ðð‘¾), ð‘© = ð‘© âˆ’ ðœ¶ (ðð‘ªð’ð’”ð’•/ðð‘©)\n",
        "\n",
        "where, ðœ¶ = learning rate\n",
        "\n",
        "(ðð‘ªð’ð’”ð’•/ðð‘¾) = derivative of cost with respect to weight\n",
        "\n",
        "(ðð‘ªð’ð’”ð’•/ðð‘©) = derivative of cost with respect to bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ref: \n",
        "\n",
        "[1]https://www.analyticsvidhya.com/blog/2021/05/logistic-regression-supervised-learning-algorithm-for-classification/\n",
        "\n",
        "[2]https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#. \n",
        "\n",
        "[3]https://avabodha.in/logistic-regression-and-basics-of-neural-network/"
      ],
      "metadata": {
        "id": "gWqYfnzdXCUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMbsjQ8tVSg6"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from random import random\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from skimage import color\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regresssion implemenation from Scratch\n",
        "class LR:\n",
        "  #Coded by Nehal Malap- 21237207 \n",
        "  def __init__(self, alpha, max_iters):\n",
        "        self.alpha = alpha\n",
        "        self.max_iters = max_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "  def _sigmoid(self, z):\n",
        "    op = 1 / (1 + np.exp(-z))\n",
        "    return op\n",
        " \n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    # init parameters\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "    \n",
        "    def loss_function(y, y_hat):\n",
        "      J = -(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
        "      return J\n",
        "\n",
        "    def gradient_descent(X, y, y_hat):\n",
        "      dw = np.dot((X), (y_hat - y))\n",
        "      db = np.sum((y_hat - y)) \n",
        "      return dw, db\n",
        "    \n",
        "        \n",
        "    for i in range(self.max_iters):\n",
        "      n= X.sample().index[0]\n",
        "      # approximate y with linear combination of weights and x, plus bias\n",
        "      z = np.dot(X.loc[n], self.weights) + self.bias\n",
        "     \n",
        "      \n",
        "      # apply sigmoid function\n",
        "      y_hat = self._sigmoid(z)\n",
        "      \n",
        "      #loss function\n",
        "      l = loss_function(y[n], y_hat)\n",
        "      l = np.squeeze(l)\n",
        "      loss_log =[]\n",
        "      if i % 200 == 0:\n",
        "        loss_log.append(l)\n",
        "        print(\"Loss after {} iterations: {:.3f}\".format(i, l))\n",
        "   \n",
        "     \n",
        "      # Getting the gradients of loss w.r.t parameters.\n",
        "      dw, db = gradient_descent(X.loc[n], y[n], y_hat)\n",
        "      \n",
        "      # update parameters\n",
        "      self.weights -= self.alpha * dw\n",
        "      self.bias -= self.alpha * db\n",
        "    return y,y_hat  \n",
        "\n",
        "  def predict(self, X, label): \n",
        "    z = np.dot(X, self.weights) + self.bias\n",
        "    y_hat = self._sigmoid(z)\n",
        "    y_hat_cls = [1 if i > 0.5 else 0 for i in y_hat]\n",
        "    accuracy = accuracy_score(y_hat_cls,label)\n",
        "    print(accuracy*100)\n",
        "    return np.array(y_hat_cls)\n",
        "      "
      ],
      "metadata": {
        "id": "EnjOsUOfWcr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2 :  Test on Easy Tasks**\n",
        "To accomplish the logistic Regression task, two datasets below were given.\n",
        "\n",
        "**Blobs Dataset**: The blobs300 dataset has 300 instances and five features (X1, X2, X3, X4, and class). Using Sklearns train_test_split library, we split the data into train and test set like 70% for the main training set(210 instances), and the remaining 30% is divided into validation and test set like 15% for validation test(45 instances), which we have used for tuning by experimenting different learning rate. The final is testing set for performance evaluation, which is 15% for (45 instances). \n",
        "\n",
        "**Circle Dataset**: The circle 600 dataset has 600 instances and three features (X1, X2, and class. Similarly, we spit the cirlce600 dataset . 70% for the main training set(420 instances) and the remaining 30% is again divided into validation and test set like 15% for validation test(90 instances), which we have used for tuning to experiment on different learning rates. The final is testing set for performance evaluation, which is 15% for (90 instances).\n",
        "\n",
        "**Results and Observations**: We have fitted our model on the Blobs training set. On passing the blobs validation dataset and keeping alpha 0.001, the Accuracy of the Logistic Regression model was 100% over 5000 max_iterations.\n",
        "On giving testing set, the Accuracy of the model is 97.7%.\n",
        "\n",
        "The Accuracy of the LR model for the circle 600 validation dataset is 52.2% over 5000 iterations and 0.001 alpha rate. \n",
        "The Accuracy of the LR model for the testing set is 43.33% over 5000 iterations and 0.001 alpha rate.\n"
      ],
      "metadata": {
        "id": "IfO91Sw6XdNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "#Loading the dataset for blobs \n",
        "data=pd.read_csv(\"blobs300.csv\")\n",
        "\n",
        "#Splitting the data into train and test\n",
        "X = data.drop(labels = \"Class\", axis = 1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "\n",
        "#Splitting the dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "#Splitting the test dataset further into validation and test\n",
        "x_val, x_test1, y_val, y_test1 = train_test_split(x_test , y_test, train_size = 0.5, test_size = 0.5, shuffle = True)"
      ],
      "metadata": {
        "id": "3j9ts6ZAXY4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "#Loading the dataset for circles \n",
        "data=pd.read_csv(\"circles600.csv\")\n",
        "\n",
        "#Splitting the data into train and test\n",
        "X = data.drop(labels = \"Class\", axis = 1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "\n",
        "#Splitting the dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "#Splitting the test dataset further into validation and test\n",
        "x_val, x_test1, y_val, y_test1 = train_test_split(x_test , y_test, train_size = 0.5, test_size = 0.5, shuffle = True)"
      ],
      "metadata": {
        "id": "beL5AZqJX2Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "#training the model\n",
        "LRModel = LR(alpha=0.001, max_iters=5000)\n",
        "LRModel.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "kZyTcffdYIZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1341bcc3-ef86-42e6-d310-9feeb3ec122a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 0 iterations: 0.693\n",
            "Loss after 200 iterations: 0.464\n",
            "Loss after 400 iterations: 0.683\n",
            "Loss after 600 iterations: 0.417\n",
            "Loss after 800 iterations: 0.952\n",
            "Loss after 1000 iterations: 0.501\n",
            "Loss after 1200 iterations: 0.252\n",
            "Loss after 1400 iterations: 0.315\n",
            "Loss after 1600 iterations: 0.883\n",
            "Loss after 1800 iterations: 0.355\n",
            "Loss after 2000 iterations: 0.325\n",
            "Loss after 2200 iterations: 0.248\n",
            "Loss after 2400 iterations: 0.128\n",
            "Loss after 2600 iterations: 0.141\n",
            "Loss after 2800 iterations: 0.079\n",
            "Loss after 3000 iterations: 0.582\n",
            "Loss after 3200 iterations: 0.047\n",
            "Loss after 3400 iterations: 0.428\n",
            "Loss after 3600 iterations: 0.074\n",
            "Loss after 3800 iterations: 0.309\n",
            "Loss after 4000 iterations: 0.418\n",
            "Loss after 4200 iterations: 0.023\n",
            "Loss after 4400 iterations: 0.207\n",
            "Loss after 4600 iterations: 0.126\n",
            "Loss after 4800 iterations: 0.045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25     0\n",
              " 159    0\n",
              " 140    0\n",
              " 95     0\n",
              " 278    1\n",
              "       ..\n",
              " 30     0\n",
              " 286    1\n",
              " 297    0\n",
              " 16     1\n",
              " 77     1\n",
              " Name: Class, Length: 210, dtype: int64, 0.5220493609884721)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "#Validation \n",
        "\n",
        "LRPred = LRModel.predict(x_val,y_val)"
      ],
      "metadata": {
        "id": "K0HH239tYQlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d837cd6-bc19-417f-f8f9-6358128d4e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "#Test \n",
        "LRPred = LRModel.predict(x_test1, y_test1)"
      ],
      "metadata": {
        "id": "cMK4vT_5YUT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b7b051-194f-42e7-be35-dfa6c9e6fba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.77777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3: Implementation of shallow Neural Network**\n",
        "\n",
        "**Shallow Neural Network** It is a supervised learning algorithm. SNN contains only 1 or 2 hidden layers. with the help of SNN we can understand the working of deep neural network. In the task 3 we have implement SNN with single hidden layer, single input layer and single output layer. Given an input node it process with its weight and bias value and returns the output. The output of the nodes of input layer is passes as an input to the subsequent layer.[5]\n",
        "\n",
        "**Forward Propagtion: **\n",
        "In the feed-forward part of a neural network, predictions are made based on the values in the input nodes and the weights[4]. The forward propgation starts with calculating the dot product of input feature matrix and weights matrix and passed the result to the activation function(sigmoid function). The output of the sigmoid function is y_hat value which is predicted value for the input feature.\n",
        "\n",
        "**Backward Propagation**\n",
        "In the backward propogation we started by intialising the random values of weight and bias, then compared the predicted output of the network with the actual output. After this step we trainined our model to get the predicted output value as same as or closer to the actual output. We define the cost function to calculate the difference between predicted output and actual output.Further, we calculted the derivative of cost function with respect to weight and multipy it with the difference of predicted and actual output value.\n",
        "\n",
        "**Results and Observation**\n",
        "We have fitted our model on the Blobs training set. On passing the blobs validation dataset and keeping alpha 0.001, the Accuracy of the Logistic Regression model was 100% over 5000 max_iterations. On giving testing set, the Accuracy of the model is 97.77%.\n",
        "\n",
        "The Accuracy of the LR model for the circle 600 validation dataset is 85.55% over 5000 iterations and 0.001 alpha rate. The Accuracy of the LR model for the testing set is 78.88% over 5000 iterations and 0.001 alpha rate.\n",
        "\n",
        "\n",
        "Ref:\n",
        "\n",
        "[4]https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/\n",
        "\n",
        "[5]https://towardsdatascience.com/shallow-neural-networks-23594aa97a5"
      ],
      "metadata": {
        "id": "PVZY97rQYdoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "class SNN:\n",
        "  def __init__(self, alpha, max_iters):\n",
        "          self.alpha = alpha\n",
        "          self.max_iters = max_iters\n",
        "          self.wh = None\n",
        "          self.bh = None\n",
        "          self.wout = None\n",
        "          self.bout = None\n",
        "         \n",
        "\n",
        "  # defining the Sigmoid Function\n",
        "  def _sigmoid(self,z):\n",
        "      op = 1 / (1 + np.exp(-z))\n",
        "      return op\n",
        "\n",
        "  # derivative of Sigmoid Function\n",
        "  def derivatives_sigmoid(self,z):\n",
        "      return z * (1 - z)\n",
        "\n",
        "  #Loss Function\n",
        "  def loss_function(self, y, y_hat):\n",
        "        J = -np.sum(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
        "        return J\n",
        "\n",
        "  #Weight initilization function\n",
        "  def weight_initiliaze(self, ip_nodes,hl_nodes,op_node):\n",
        "    self.wh=np.random.randn(ip_nodes,hl_nodes) \n",
        "    self.bh=np.zeros((1,hl_nodes))\n",
        "    self.wout=np.random.randn(hl_nodes,op_node) \n",
        "    self.bout=np.zeros((1,op_node))\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.weight_initiliaze(X.shape[1],X.shape[1],1)\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "   \n",
        "# training the model\n",
        "    for i in range(self.max_iters):\n",
        "            X, y = shuffle(X, y)\n",
        "\n",
        "            for j in range(X.shape[0]):\n",
        "                \n",
        "                \n",
        "                X1= X[j].reshape(1,X.shape[1])\n",
        "                y1 = y[j].reshape(-1,1)\n",
        "               \n",
        "                hl_ip= X1 @ self.wh  + self.bh\n",
        "            \n",
        "                hl_af = self._sigmoid(hl_ip)\n",
        "                opl_ip= hl_af @ self.wout + self.bout\n",
        "            \n",
        "                y_hat = self._sigmoid(opl_ip)\n",
        "              \n",
        "          \n",
        "            #Backpropagation\n",
        "                E_op = y_hat - y1\n",
        "                der_op = self.derivatives_sigmoid(y_hat)\n",
        "                der_hl = self.derivatives_sigmoid(hl_af)\n",
        "                d_op= E_op * der_op\n",
        "          \n",
        "            \n",
        "                E_hl = d_op.dot(self.wout.T)\n",
        "                d_hl = E_hl * der_hl\n",
        "          \n",
        "                self.wout -= hl_af.T.dot(d_op)*self.alpha\n",
        "                self.bout -= np.sum(d_op,axis=0)*self.alpha\n",
        "                self.wh -= X1.T.dot(d_hl)*self.alpha\n",
        "                self.bh -= np.sum(d_hl,axis=0)*self.alpha\n",
        "\n",
        "            l = self.loss_function(y1, y_hat)\n",
        "            l = np.squeeze(l)\n",
        "            loss_log =[]\n",
        "            if i % 500 == 0:\n",
        "              loss_log.append(l)\n",
        "              print(\"Loss after {} iterations: {:.3f}\".format(i, l))\n",
        " \n",
        "  def predict(self, X, label):\n",
        "    z = np.dot(X, self.wh) + self.bh\n",
        "    a1 = self._sigmoid(z)\n",
        "    z2 = np.dot(a1, self.wout) + self.bout\n",
        "    a2 = self._sigmoid(z2)\n",
        "    y_hat_cls = [1 if i > 0.5 else 0 for i in a2]\n",
        "    accuracy = accuracy_score(y_hat_cls,label)\n",
        "    print(accuracy*100)\n",
        "    \n",
        "    return np.array(y_hat_cls)       "
      ],
      "metadata": {
        "id": "5FFZhkObtvsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Loading the dataset for blobs \n",
        "data=pd.read_csv(\"blobs300.csv\")\n",
        "\n",
        "#Splitting the data into train and test\n",
        "X = data.drop(labels = \"Class\", axis = 1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "#Splitting the dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "#Splitting the test dataset further into validation and test\n",
        "x_val, x_test1, y_val, y_test1 = train_test_split(x_test , y_test, train_size = 0.5, test_size = 0.5, shuffle = True)"
      ],
      "metadata": {
        "id": "LqWJonJQZbcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Loading the dataset for circles \n",
        "data=pd.read_csv(\"circles600.csv\")\n",
        "\n",
        "X = data.drop(labels = \"Class\", axis = 1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "#Splitting the dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "#Splitting the test dataset further into validation and test\n",
        "x_val, x_test1, y_val, y_test1 = train_test_split(x_test , y_test, train_size = 0.5, test_size = 0.5, shuffle = True)"
      ],
      "metadata": {
        "id": "txeYCYSBZV8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#training the model\n",
        "SNNModel = SNN(alpha=0.001, max_iters=5000)"
      ],
      "metadata": {
        "id": "E_IPS6qEY4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "x_new = x_train.to_numpy(copy=True)\n",
        "y_new = y_train.to_numpy(copy=True)\n",
        "SNNModel.fit(x_new,y_new)"
      ],
      "metadata": {
        "id": "T99F3PqqZId4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb559e4a-2525-4e6b-c0db-353b036ce288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 0 iterations: 0.758\n",
            "Loss after 500 iterations: 0.780\n",
            "Loss after 1000 iterations: 0.667\n",
            "Loss after 1500 iterations: 0.668\n",
            "Loss after 2000 iterations: 0.641\n",
            "Loss after 2500 iterations: 0.844\n",
            "Loss after 3000 iterations: 0.948\n",
            "Loss after 3500 iterations: 0.463\n",
            "Loss after 4000 iterations: 0.401\n",
            "Loss after 4500 iterations: 0.299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Validation \n",
        "SNNPred = SNNModel.predict(x_val,y_val)"
      ],
      "metadata": {
        "id": "fUimRfOpZLa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fed2b41-50b5-4ae6-9d0c-932086fc60ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.55555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Test \n",
        "SNNPred = SNNModel.predict(x_test1, y_test1)"
      ],
      "metadata": {
        "id": "pX633rydZOo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bd53a1-5bf9-401b-db09-d05ed98422ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78.88888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 4:Image Recognition of given category(Dog and Automobile)**\n",
        "\n",
        "\n",
        "\n",
        "For task 4 image Recognition, we have used the sample code provided by the professor and made few changes to accomplish the task of recognizing the dog and automobile images that was assigned to us from the dataset.\n",
        "\n",
        "The given datatset contains 60k images of airplane, automobile, bird, cat, deer dog, frog, horse, sheep and truck. The dataset is divided into train(50K) and test(10K). We have used batch1 which contains 10K random images. with the help of loadbatch and loadlabel function we loaded the dataset and labels into the variable. As per the required preprocessing we reshaped the every image in (3,32,32) format and transposed the image.The preprocessed image then converted into greyscale image with the help of color.rgb2gray() function.\n",
        "\n",
        "Further, we split the greyscale image data into train and validation as 70% and 30% respectively. \n",
        "\n",
        "Hence, from the batch of 10000 images, there are total 1911 images are available of dog and automobile category. hence the preprocessed image data 1911\n",
        "\n",
        "divided into train = 1337 , test = 574, valdiation test data = 374, testing data = 374\n",
        "\n",
        "We trained our model on training data, tuned our model on validation data and finally tested our model on testing data and got the below accuracy.\n",
        "\n",
        "The accuracy of the model on validation data is 68.64% over 10 maximum iterations with alpha = 0.01\n",
        "\n",
        "The accuracy of the model on test data is 80%"
      ],
      "metadata": {
        "id": "5a3_ZVJ3ZnYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function taken from the CIFAR website\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def loadbatch(batchname):\n",
        "    folder = 'cifar-10-batches-py'\n",
        "    batch = unpickle(\"/\" +\"content\" +\"/\" + folder+ \"/\"+batchname)\n",
        "    return batch    \n",
        "\n",
        "def loadlabelnames():\n",
        "    folder = 'cifar-10-batches-py'\n",
        "    meta = unpickle(\"/\"+ \"content\"+\"/\"+folder+ \"/\"+'batches.meta')\n",
        "    return meta[b'label_names'] \n"
      ],
      "metadata": {
        "id": "i9VPp4glZTtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch1 = loadbatch('data_batch_1')\n",
        "print(\"Number of items in the batch1 is\", len(batch1))\n",
        "print('All keys in the batch:', batch1.keys())"
      ],
      "metadata": {
        "id": "MVuFVX8lZ6XL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4558e1c0-3fea-4f2f-fbcd-a15ccec5ebeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items in the batch1 is 4\n",
            "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = batch1[b'data']\n",
        "labels = batch1[b'labels']\n",
        "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
        "names = loadlabelnames()"
      ],
      "metadata": {
        "id": "jhI1Fb0Dadii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67736f4b-4098-4fff-b5ab-4bf3ea160bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of data in this batch: 10000 , size of labels: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting the classes dogs and automobiles from the data-set data_batch_1 for training and validation\n",
        "#Coded by Gehna Gurbani- 21230450\n",
        "dogAuto=[]\n",
        "dogAuto_labels=[]\n",
        "for i in range(len(data)):\n",
        "  if((labels[i]== 1) or (labels[i]==5)):\n",
        "    dogAuto.append(data[i])\n",
        "    dogAuto_labels.append(labels[i])\n",
        "dogAuto_labels = [0 if  x-5 == 0 else x for x in dogAuto_labels]  "
      ],
      "metadata": {
        "id": "BOT7ai_YaeZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "DA_data = np.array(dogAuto)\n",
        "DA_labels = np.array(dogAuto_labels)\n",
        "img_grey =np.array([])\n",
        "#Converting the rgb images to grey\n",
        "for i in range (len(DA_data)):\n",
        "  pic = DA_data[i]\n",
        "  pic.shape = (3,32,32)\n",
        "  pic = pic.transpose([1, 2, 0])\n",
        "  pic_grey = color.rgb2gray(pic)"
      ],
      "metadata": {
        "id": "jT4dc8gfatyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Splitting the dataset into train and validation, normalizing the values \n",
        "x_train, x_test, y_train, y_test = train_test_split(DA_data, DA_labels, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "x_train = x_train.reshape(x_train.shape[0],32*32*3) / 255\n",
        "x_val =  x_test.reshape(x_test.shape[0],32*32*3) /255\n",
        "y_train = y_train.reshape(y_train.shape[0],1)\n",
        "y_val = y_test.reshape(y_test.shape[0],1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qn3gjyMaaxNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Coded by Gehna Gurbani- 21230450\n",
        "#training the model\n",
        "SNNModel = SNN(alpha=0.01, max_iters=10)"
      ],
      "metadata": {
        "id": "nwNH7TZSbDO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "SNNModel.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "DBqmmk_pbENY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af11c9f7-1602-4b7d-9329-6f872036055e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 0 iterations: -0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Prediction \n",
        "SNNPred = SNNModel.predict(x_val, y_val)"
      ],
      "metadata": {
        "id": "B5BxogSObIKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066c4249-afd2-4c98-bff8-45bb4cda5a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68.6411149825784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Selecting the classes dogs and automobiles from the data-set test_batch for testing\n",
        "dogAuto_test=[]\n",
        "dogAuto_labels_test=[]\n",
        "for i in range(len(data)):\n",
        "  if((labels[i]== 1) or (labels[i]==5)):\n",
        "    dogAuto_test.append(data[i])\n",
        "    dogAuto_labels_test.append(labels[i])\n",
        "dogAuto_labels_test = [0 if  x-5 == 0 else x for x in dogAuto_labels_test]     \n",
        "\n",
        "DA_data_test = np.array(dogAuto_test)\n",
        "DA_labels_test = np.array(dogAuto_labels_test)\n",
        "img_grey_test =np.array([])\n",
        "#Converting the rgb images to grey\n",
        "for i in range (len(DA_data_test)):\n",
        "  pic = DA_data_test[i]\n",
        "  pic.shape = (3,32,32)\n",
        "  pic = pic.transpose([1, 2, 0])\n",
        "  pic_grey = color.rgb2gray(pic)\n",
        "#Using the test_batch to test the model \n",
        "DA_data_test = DA_data_test.reshape(DA_data_test.shape[0],32*32*3) / 255\n",
        "DA_labels_test = DA_labels_test.reshape(DA_labels_test.shape[0],1)"
      ],
      "metadata": {
        "id": "m-DdwZN_cGrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "#Prediction\n",
        "SNNPred = SNNModel.predict(DA_data_test, DA_labels_test)"
      ],
      "metadata": {
        "id": "Nj1Jh-SRc5Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d56cbac-6e14-405d-918e-4fc62d7c1fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.11512297226582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 5: \n",
        "\n",
        "Enhancements done by Nehal Malap - 21237207\n",
        "As part of the enhancement, i have implemented normal backprop RMSprop alogorithm. Root Mean Squred propgation algorithm. It is adaptive learning rate algorithm. The aim of the RMSprop algorithm is It first intialise the weight and bias(without any superscript and subscript) to 0. In RMSPrope squared gradient is used instead of gradient hence the update formula of weight and bias includes the square root of average squared gradient.[6]\n",
        "\n",
        "The formula:\n",
        "\n",
        "w -= alpha * delta_weight/âˆšaverage gradient decent + E\n",
        "b -= alpha * delta_bias/âˆšaverage gradient decent + E\n",
        "\n",
        "where E is very small value i.e 1e-6 used to maintain numerical stability in the computation and will avoid the division by 0 or close to 0.\n",
        "\n",
        "Ref: \n",
        "[6]https://d2l.ai/chapter_optimization/rmsprop.html"
      ],
      "metadata": {
        "id": "b_dXTzJdbK02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Nehal Malap- 21237207 \n",
        "class RMS:\n",
        "\n",
        "    def __init__(self, num_weight, num_bias, learning_rate):  \n",
        "        self.Sw = [0] * num_weight\n",
        "        self.Sb = [0] * num_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def RMSprop(self, params, grads, bias, beta = 0.9):\n",
        "      updated_w_params = []\n",
        "      updated_b_params = []\n",
        "      \n",
        "      for i, (w_param, grad,b_param) in enumerate(zip(params, grads)):\n",
        "        self.Sw[i] = beta * self.Sw[i] + (1-beta) * grad **2\n",
        "        self.Sb[i] = beta * self.Sb[i] + (1-beta) * bias **2\n",
        "        w_param += - self.learning_rate * grad / (np.sqrt(self.Sw[i])+ 1e-6)\n",
        "        b_param += - self.learning_rate * bias / (np.sqrt(self.Sb[i])+ 1e-6)\n",
        "        updated_w_params.append(w_param)\n",
        "        updated_b_params.append(b_param)\n",
        "        \n",
        "      return updated_w_params,updated_b_params\n",
        "        "
      ],
      "metadata": {
        "id": "JqGByJTlG1q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 5 : Enhancements done by Gehna Gurbani- 21230450 As a part of enhancement I am implementing a neural net with multiple hidden layers and adam optimizer.\n",
        "Adaptive Moment Estimation (Adam) is an approach for calculating adaptive learning rates for each parameter. At the start for each weight W and Bias b the terms V and S are initialized to 0. \n",
        "After the backpropogation step , the V and the S terms are updates and the correction is calculated. The gradient descent update step is carried out with these new terms.\n",
        "Also, the decay rate Beta1 and Beta2 are very smallclose to 1.\n",
        "The proposed value for Beta1 is  0.9 and that for Beta2 is 0.999, whcih gives the optimal results.[7]\n",
        "\n",
        "Ref: [7] https://ruder.io/optimizing-gradient-descent/index.html#minibatchgradientdescent\n",
        "\n"
      ],
      "metadata": {
        "id": "sYvi09A9UUhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_config(X, Y, n_h=[128, 64, 16, 8]):\n",
        "   n_x = X.shape[1] # size of input layer\n",
        "   n_y = Y.shape[1] # size of output layer\n",
        "   dims = sum([[n_x], n_h, [n_y]], [])\n",
        "   K = len(dims) # number of network layers\n",
        "   params = {}\n",
        "   for k in range(1, K):\n",
        "        params['W{}'.format(k)] = np.random.randn(dims[k-1],dims[k]) * 1/np.sqrt(dims[k-1])\n",
        "        params['b{}'.format(k)] = np.zeros((1,dims[k]))   \n",
        "        return params    "
      ],
      "metadata": {
        "id": "FxmjujhaUOxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fwd(W, b, A):\n",
        "   Z = A@W + b\n",
        "   cache = {\"W\": W, \"b\": b, \"A_prev\": A}\n",
        "   return Z, cache"
      ],
      "metadata": {
        "id": "bUEgZh7pZQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  A = 1/(1 + np.exp(-Z))\n",
        "  cache = (Z)\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "70offn41ZLx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(params, X, Y):\n",
        "   caches = []\n",
        "   K = len(params) // 2\n",
        "\n",
        "   A = X\n",
        "   for k in range(1, K+1):\n",
        "        A_prev = A\n",
        "  \n",
        "        # Linear Hypothesis\n",
        "        Z =   A_prev @ params['W'+str(k)] + params['b'+str(k)] \n",
        "        # Storing the linear cache\n",
        "        linear_cache = (A_prev, params['W'+str(k)], params['b'+str(k)])\n",
        "        # Applying sigmoid on linear hypothesis\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "        # storing the both linear and activation cache\n",
        "        cache = (linear_cache, activation_cache)\n",
        "        caches.append(cache)\n",
        "      \n",
        "\n",
        "        loss = float('nan')\n",
        "        if Y is not None:\n",
        "          Y_hat = A\n",
        "          n = Y.shape[0]\n",
        "          # Compute the cross-entropy loss\n",
        "         \n",
        "          loss = -(1 / n) * (Y.T @ np.log(Y_hat) + (1-Y.T) @ np.log(1-Y_hat))\n",
        "          loss = np.squeeze(loss)\n",
        "   return A, loss, caches"
      ],
      "metadata": {
        "id": "R6Y3LAIWXu-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def singlelayer_back(dA, cache):\n",
        "   linear_cache, activation_cache = cache\n",
        "   dZ=[]\n",
        "   Z = activation_cache\n",
        "   S = 1/(1+np.exp(-Z))\n",
        "   for i in range(len(S)):\n",
        "      dz = dA*sigmoid(Z[i])*(1-sigmoid(Z[i])) # The derivative of the sigmoid function\n",
        "      dZ=dZ.append(dz)\n",
        "   A_prev, W, b = linear_cache\n",
        "   m = A_prev.shape[1]   \n",
        "   \n",
        "   dW = (1/m)*np.dot(dZ, A_prev.T)\n",
        "   db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "   dA_prev = np.dot(W.T, dZ)\n",
        "  \n",
        "   return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "EUSX5ElOX8gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(AK, Y, caches):\n",
        "  grads = {}\n",
        "  K = len(caches)\n",
        "  n = AK.shape[1]\n",
        "\n",
        "  dAK = - (np.divide(Y, AK) - np.divide(1 - Y, 1 - AK))\n",
        "\n",
        "  cache = caches[K-1]\n",
        "  grads[\"dW{}\".format(K-1)], grads[\"db{}\".format(K-1)],  grads[\"dA{}\".format(K-1)] = singlelayer_back(dAK, cache)\n",
        "  for k in reversed(range(K - 1)):\n",
        "    \n",
        "    cache = caches[k]\n",
        "    dA_prev_temp, dW_temp, db_temp = singlelayer_back(grads[\"dA\" + str(k+1)], cache)\n",
        "    grads[\"dA\" + str(k)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(k + 1)] = dW_temp\n",
        "    grads[\"db\" + str(k + 1)] = db_temp\n",
        "  return grads"
      ],
      "metadata": {
        "id": "g7CH1EP4YBrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def update_params(params, grads, learning_rate=0.8):\n",
        "  K = len(params) // 2\n",
        "  for k in range(K):\n",
        "    params['W{}'.format(k+1)] = params[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
        "    params['b{}'.format(k+1)] = params[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
        "    return params"
      ],
      "metadata": {
        "id": "tV3PnC2mYGK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fit(params, X, Y, epochs=2500, learning_rate=0.0075, verbose=False):\n",
        "    loss_log = []\n",
        "    for i in range(epochs):\n",
        "        A, loss, caches = forward_prop(params, X, Y)\n",
        "        grads = back_prop(A, Y, caches)\n",
        "        params = update_params(params, grads, learning_rate)      \n",
        "        # logs\n",
        "        if i % 200 == 0:\n",
        "            loss_log.append(loss.item())\n",
        "            if verbose:\n",
        "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
        "     \n",
        "    return params, grads, loss_log"
      ],
      "metadata": {
        "id": "8NmOEe4gYWJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(params, X):\n",
        "    AK, _, _ =  forward_prop(params,X)\n",
        "    Y_hat = np.where(AK<0.5,0,1) # Convert activations to {0,1} predictions   \n",
        "    return Y_hat"
      ],
      "metadata": {
        "id": "AV6vU_bPYXST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 7, 3], epochs=2500, learning_rate=0.007, verbose=True):\n",
        "    params = model_config(X_train, Y_train, hidden_layers)\n",
        "    params, grads, loss = model_fit(params, X_train, Y_train, epochs, verbose=True)\n",
        "  \n",
        "    Y_hat_train = model_predict(params, X_train)\n",
        "    Y_hat_test = model_predict(params, X_test)\n",
        "    train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
        "    test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test))) \n",
        "\n",
        "    print(\"{:.1f}% training acc.\".format(train_acc))\n",
        "    print(\"{:.1f}% test acc.\".format(test_acc))\n",
        "        \n",
        "    return {\"PARAMS\": params, \"LOSS\": loss, \"GRADS\": grads, \"ACC\": [train_acc, test_acc], \"LR\": learning_rate}"
      ],
      "metadata": {
        "id": "ujMkflWxYaxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizers:\n",
        "    def __init__(self, num_weight, learning_rate):\n",
        "        self.m = [0] * num_weight\n",
        "        self.v = [0] * num_weight\n",
        "        self.t = 1\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def Adam(self, params, grads, beta1 = 0.9,beta2 = 0.999):\n",
        "        updated_params = []\n",
        "        \n",
        "        for  i, (param, grad) in enumerate(zip(params, grads)):\n",
        "          \n",
        "          self.m[i] = beta1 * self.m[i] + (1-beta1) * grad          \n",
        "          self.v[i] = beta2 * self.v[i] + (1-beta2) * grad **2\n",
        "          m_corrected = self.m[i] / (1-beta1**self.t)\n",
        "          v_corrected = self.v[i] / (1-beta2**self.t)\n",
        "          param += -self.learning_rate * m_corrected / (np.sqrt(v_corrected) + 1e-8)\n",
        "          updated_params.append(\n",
        "            param \n",
        "          )\n",
        "          \n",
        "        self.t +=1\n",
        "        \n",
        "        return updated_params"
      ],
      "metadata": {
        "id": "jINrSEiZhMz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coded by Gehna Gurbani- 21230450\n",
        "x_train, x_test, y_train, y_test = train_test_split(DA_data, DA_labels, train_size = 0.70, test_size = 0.30, shuffle = True)\n",
        "x_train = x_train.reshape(x_train.shape[0],32*32*3) / 255\n",
        "x_val =  x_test.reshape(x_test.shape[0],32*32*3) /255\n",
        "y_train = y_train.reshape(y_train.shape[0],1)\n",
        "y_val = y_test.reshape(y_test.shape[0],1)\n"
      ],
      "metadata": {
        "id": "_lgnCF-6Zn7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acknowledgements:\n",
        "\n",
        "\n",
        "*   TASK 1 and 2: Coded and Explained by Nehal Malap \n",
        "*   TASK 3 and 4 : Coded and Explained by Gehna Gurbani \n",
        "*   TASK 5 : Done individully.\n",
        "\n",
        "\n",
        "\n",
        "--EOF--\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7h5_LSdjnF6u"
      }
    }
  ]
}